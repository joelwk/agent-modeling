{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "import json\n",
    "import logging\n",
    "\n",
    "model = GPT4All(r\"/mnt/d/models/mistral-7b-openorca.gguf2.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple chat workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "User: hello\n",
      "Assistant: Hello! How can I assist you today?\n",
      "User: What is the optimal prompt for generating python code?\n",
      "Assistant: To generate Python code, a good starting point would be to provide a clear and concise description of what you want the code to accomplish. For example: \"Generate a function that takes two numbers as input and returns their sum.\" This will help me understand your desired output and create an appropriate prompt for generating the required Python code.\n",
      "User: thank you\n",
      "Assistant: You're welcome! If you have any specific requirements or additional details, please feel free to provide them so I can generate a more accurate Python code snippet for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def format_chat(session_data):\n",
    "    formatted_output = []\n",
    "    for entry in session_data:\n",
    "        role = entry['role']\n",
    "        content = entry['content'].strip()\n",
    "        formatted_output.append(f\"{role.capitalize()}: {content}\\n\")\n",
    "    return \"\".join(formatted_output)\n",
    "\n",
    "def simple_chat():\n",
    "    with model.chat_session():\n",
    "        response1 = model.generate(prompt='hello', temp=0)\n",
    "        response2 = model.generate(prompt='What is the optimal prompt for generating python code?', temp=0)\n",
    "        response3 = model.generate(prompt='thank you', temp=0)\n",
    "        \n",
    "        # Get and format the current chat session\n",
    "        session_data = model.current_chat_session\n",
    "        formatted_output = format_chat(session_data)\n",
    "        \n",
    "        # Print the formatted chat session\n",
    "        print(formatted_output)\n",
    "        \n",
    "simple_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Force introspection chat workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI-powered Geography Expert, capable of providing information and answering questions related to geographical data, locations, and facts.\n",
      "It's difficult for me to pick favorites as there are so many fascinating mountains around the world. However, some notable ones include Mount Everest (the highest peak), Kilimanjaro (the tallest free-standing mountain) and Denali (North America's highest peak).\n",
      "System: You are a geography expert.\n",
      "Be terse.\n",
      "User: who are you?\n",
      "Assistant: I am an AI-powered Geography Expert, capable of providing information and answering questions related to geographical data, locations, and facts.\n",
      "User: what are your favorite 3 mountains?\n",
      "Assistant: It's difficult for me to pick favorites as there are so many fascinating mountains around the world. However, some notable ones include Mount Everest (the highest peak), Kilimanjaro (the tallest free-standing mountain) and Denali (North America's highest peak).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def format_session(session_data):\n",
    "    formatted_output = []\n",
    "    for entry in session_data:\n",
    "        role = entry['role']\n",
    "        content = entry['content'].strip()\n",
    "        formatted_output.append(f\"{role.capitalize()}: {content}\\n\")\n",
    "    return \"\".join(formatted_output)\n",
    "\n",
    "def introspect_chat():\n",
    "    with model.chat_session('You are a geography expert.\\nBe terse.', '### Instruction:\\n{0}\\n\\n### Response:\\n'):\n",
    "        response1 = model.generate(prompt='who are you?', temp=0)\n",
    "        print(response1)\n",
    "        response2 = model.generate(prompt='what are your favorite 3 mountains?', temp=0)\n",
    "        print(response2)\n",
    "        \n",
    "        # Get and format the current chat session\n",
    "        session_data = model.current_chat_session\n",
    "        formatted_output = format_session(session_data)\n",
    "        \n",
    "        # Print the formatted chat session\n",
    "        print(formatted_output)\n",
    "\n",
    "introspect_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
